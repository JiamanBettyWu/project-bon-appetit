{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the packages used \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import requests \n",
    "import requests_cache\n",
    "import time\n",
    "import lxml.html as lx\n",
    "import xlwt \n",
    "\n",
    "requests_cache.install_cache(\"yelp\")\n",
    "\n",
    "import plotnine\n",
    "from plotnine import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get all the businesses in New York \n",
    "def read_key(keyfile):\n",
    "    with open(keyfile) as f:\n",
    "        return f.readline().strip(\"\\n\")\n",
    "    \n",
    "# read the files: \n",
    "key = read_key(\"../Documents/yelp_api_2.txt\")\n",
    "headers = {'Authorization': 'Bearer %s' % key}\n",
    "\n",
    "\n",
    "# the function to get the data with imput location and number of results\n",
    "def get_business(loc, num):\n",
    "    \"\"\"\n",
    "    This function takes a state, for example: New York, and the number of restaurants needed from that location.\n",
    "    And it returned a dataframe the information obtained for all the reataurants. \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    n = 0\n",
    "    while n <= num: \n",
    "        headers = {'Authorization': 'Bearer %s' % key}\n",
    "        url = \" https://api.yelp.com/v3/businesses/search\"\n",
    "        try: \n",
    "            req = requests.get(url,  headers = headers, params = {\n",
    "                         \"term\": \"restaurants | food\", \n",
    "                         \"location\": loc,\n",
    "                         \"limit\": 50,\n",
    "                         \"offset\": n})\n",
    "            result = pd.DataFrame(req.json()[\"businesses\"])\n",
    "        except KeyError:\n",
    "            break\n",
    "        df = df.append(result, ignore_index = True)\n",
    "        df[\"state\"] = loc\n",
    "        n += 50\n",
    "        print(n)  \n",
    "        print(loc)\n",
    "    return (df)\n",
    "\n",
    "# Get the information of the first 1000 restaurants in the New York City and save it in a csv file.  \n",
    "ny_rst = get_business(\"New York\", 1000)\n",
    "ny_rst.to_csv(r'ny_rst.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the information from the saved file and get the urls for the first 1000 restaurants. \n",
    "ny_rst = pd.read_csv(\"ny_rst.csv\")\n",
    "ny_url = ny_rst[\"url\"]\n",
    "\n",
    "def check_url(link):\n",
    "    \"\"\"\n",
    "    This function take a link and return the status. If 200 is returned, the link is working. \n",
    "    \"\"\"\n",
    "    req = requests.get(link)\n",
    "    time_fake = np.random.uniform(low=0.5, high=1.5)\n",
    "    time.sleep(time_fake)\n",
    "    x = 'The status code is {}'.format(req.status_code)\n",
    "    print(time_fake)\n",
    "    return x\n",
    "\n",
    "# check all urls are valid, no 404 errors. \n",
    "result = [check_url(ny_url.iloc[i]) for i in range(len(ny_url))] # all valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to scrape the information of each restaurant:\n",
    "\n",
    "ny_rst = pd.read_csv(\"ny_rst.csv\")\n",
    "ny_url = ny_rst[\"url\"]\n",
    "ny_id = ny_rst[\"id\"]\n",
    "\n",
    "def save_html(url):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes url, makes get request, then parse and return html\n",
    "    \"\"\"\n",
    "    \n",
    "    sleeptime = np.random.uniform(low = 0.5, high = 1.5)\n",
    "    time.sleep(sleeptime)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    html = lx.fromstring(response.text)\n",
    "    html.make_links_absolute(url)\n",
    "    \n",
    "    return html\n",
    "\n",
    "def one_row(row):\n",
    "    \"\"\"\n",
    "    This function takes in one row of html table, and return a tuple with of length two. Position one is the \n",
    "    attribute, and position two is the value.\n",
    "    \"\"\"\n",
    "    return row.xpath(\".//dt\")[0].text_content().strip(), row.xpath(\".//dd\")[0].text_content().strip()\n",
    "\n",
    "\n",
    "def save_review(html):\n",
    "    \"\"\"\n",
    "    This funtion get all customer reviews for one html.\n",
    "    \n",
    "    \"\"\"\n",
    "    links = html.xpath(\"//p[@lang  = 'en']\")\n",
    "    result = [l.text_content().strip() for l in links]\n",
    "    return result\n",
    "\n",
    "def get_everything(html, biz_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in html and id. It return a dictionary with restaurant reviews, attributes, review ratings,\n",
    "    and biz_id for a given restaurant.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get reviews\n",
    "    review = save_review(html)\n",
    "    \n",
    "    # get attributes\n",
    "    tab = html.xpath(\"//div[@class  = 'ywidget']//div[contains(@class, 'short-def-list')]\")\n",
    "    \n",
    "    if len(tab) == 0:\n",
    "        attrib = \"biz_id\", biz_id\n",
    "    else:\n",
    "        tab = tab[0]\n",
    "        attrib = [one_row(r) for r in tab]\n",
    "        item = \"biz_id\", biz_id\n",
    "        attrib.append(tuple(item))\n",
    "        \n",
    "    # get review rating     \n",
    "    review_rating = html.xpath(\"//div[@class = 'biz-rating biz-rating-large clearfix']/div//div/@title\")\n",
    "    \n",
    "    return {\"attributes\" : attrib, \"review\" : review, \"review rating\" : review_rating, \"biz_id\": biz_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final version\n",
    "\n",
    "html = [save_html(u) for u in ny_url[0:1000]]\n",
    "everything = [get_everything(html[i], ny_id[1000 + i]) for i in range(len(html))]\n",
    "everything = pd.DataFrame(everything)\n",
    "everything.to_csv(r'everything.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv(\"/Users/apple/Desktop/everything.csv\")\n",
    "reviews = total['review']\n",
    "bus_id = total['biz_id']\n",
    "\n",
    "import nltk\n",
    "import xlrd\n",
    "import string\n",
    "import nltk.corpus\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "import nltk.corpus as corpus\n",
    "nltk.download('vader_lexicon')\n",
    "import re\n",
    "\n",
    "stopwords = corpus.stopwords.words(\"english\")\n",
    "\n",
    "import ast \n",
    "from statistics import mean\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def get_adj_and_adv(text):\n",
    "    \"\"\"\n",
    "    This functionis to firstly tokenize the words and then select\n",
    "    the words that is tagged as adverbe and adjective\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    text_token = word_tokenize(text_lower)\n",
    "    result_tags = nltk.pos_tag(text_token)\n",
    "    \n",
    "    words = [(word) for word, tag in result_tags if tag in ('JJ','RB')]\n",
    "    return (words)\n",
    "\n",
    "def get_noun(text):\n",
    "    \"\"\"\n",
    "    This functino is to tokenize the words and select the words\n",
    "    that is tagged as noun\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    text_token = word_tokenize(text_lower)\n",
    "    result_tags = nltk.pos_tag(text_token)\n",
    "    \n",
    "    words = [(word) for word, tag in result_tags if tag in ('NN')]\n",
    "    return (words)\n",
    "\n",
    "def nltk_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    This function is to process the sentiment on each tokenized sentences\n",
    "    and then generate a sentiment value for each sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_compound(review_total_restaurant):\n",
    "    \"\"\" \n",
    "        This function is to get the compound value, which is the weight value of negative, positive and netural words\n",
    "        for each restaurant\n",
    "    \"\"\"\n",
    "    if len(review_total_restaurant) == 0: # test if there is a review if not, the value would return 0\n",
    "        x2 = 0\n",
    "    else:\n",
    "        x = [nltk_sentiment(review_total_restaurant[i]) for i in range(len(review_total_restaurant))] \n",
    "        # get the three values for each review sentences\n",
    "        x1 = [(list(x[i].items())[-1][1]) for i in range(len(x)) ]\n",
    "        # get the compund view of each reivew\n",
    "        x2 = mean(x1)\n",
    "        #calculate the average of the compound value\n",
    "    return x2\n",
    "\n",
    "def get_biz_id(reviews):\n",
    "    \"\"\"\n",
    "    This function is to get a bussiness id for each review\n",
    "    \"\"\"\n",
    "    total_rest = [ast.literal_eval(reviews[i]) for i in range(len(reviews))]\n",
    "    test = [(len(total_rest[i]) *  te[i]) for i in range(len(te))]\n",
    "    split_mul_string = [splitstring(test[i]) for i in range(len(test))]\n",
    "    sp_mul_str = list(itertools.chain.from_iterable(split_mul_string))\n",
    "    result = pd.DataFrame(sp_mul_str)\n",
    "    return result\n",
    "\n",
    "testing666 = pd.DataFrame(total_rest)\n",
    "# we first conver the reviews to a dataframe\n",
    "testing667 = testing666[testing666.columns[1:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "# Then we join all the colouns (reviews) of each row(restruant) to\n",
    "# a one colmun dataframe\n",
    "\n",
    "testing668 = pd.DataFrame(testing667)\n",
    "# new we convert the combined reviews as a dataframe\n",
    "\n",
    "testing668 = testing668.rename(columns={0: \"review\"})\n",
    "# rename it \n",
    "\n",
    "bus_id.reset_index(drop=True, inplace=True)\n",
    "testing668.reset_index(drop=True, inplace=True)\n",
    "\n",
    "testing669 = pd.concat([testing668, bus_id], axis=1)\n",
    "\n",
    "rew_tok = [sent_tokenize(testing667[i]) for i in range(len(testing667))]\n",
    "# now tokenize each sentences of the review to parper for the \n",
    "#sentiment analysis\n",
    "x2 = [get_compound(rew_tok[i]) for i in range(len(rew_tok))]\n",
    "# get the average sentiment analysis result for each resutrant\n",
    "\n",
    "compd =pd.DataFrame(x2)\n",
    "compd = compd.rename(columns={0: \"Score\"})\n",
    "#Convert the sentiment analysis result to a dataframe\n",
    "\n",
    "compd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_review_w_score = pd.concat([testing669,compd],axis = 1) \n",
    "#join it as a dataframe \n",
    "# the dataframe contains the combined reviews for each restruant and the \n",
    "#average sentiment scores for each restruant reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev =  [re.sub(r'[^\\w\\s]','',reviews[i]) for i in range(len(reviews))] # tokenize by words\n",
    "# now to tokenize the scentence\n",
    "\n",
    "review_noun = [get_noun(rev[i]) for i in range(len(rev))] # get the noun for \n",
    "\n",
    "review_adj = [get_adj_and_adv(rev[i]) for i in range(len(rev))]\n",
    "\n",
    "review_noun_total = [j for i in review_noun for j in i]\n",
    "# combine the nouns to one list\n",
    "review_adj_total = [j for i in review_adj for j in i]\n",
    "#combine the adjs to one list\n",
    "\n",
    "text_token = [word_tokenize(rev[i]) for i in range(len(rev))]\n",
    "\n",
    "text_token_total = [j for i in text_token for j in i]\n",
    "\n",
    "\n",
    "# these are the stop words we want to remove \n",
    "exclude_noun = ['i','food']\n",
    "exclude_adj = ['not','i','so','very','here','just','also','really','too','well','other','first','','restaurant','next','then','as','lo','as','again']\n",
    "review_noun_total_re_stopwords = [w for w in review_noun_total if w  not in exclude_noun ]\n",
    "\n",
    "review_adh_total_re_stopwords = [w for w in review_adj_total if w  not in exclude_adj ]\n",
    "fd_n = nltk.FreqDist(review_noun_total_re_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rest = [ast.literal_eval(reviews[i]) for i in range(len(reviews))]\n",
    "total_score_compound = [[get_compound(x) for x in total_review_token[i]] for i in range(len(total_review_token)) ]\n",
    "# this part is to get the vander value for each review and get each\n",
    "#review text\n",
    "testing26 = list(itertools.chain.from_iterable(total_rest))\n",
    "# compress the lists into one list\n",
    "testing25 = list(itertools.chain.from_iterable(total_score_compound))\n",
    "review_df = pd.DataFrame(testing26)\n",
    "review_df = review_df.rename(columns={0: \"Review\"})\n",
    "score_df = pd.DataFrame(testing25)\n",
    "score_df = score_df.rename(columns={0: \"Compound Value\"})\n",
    "testing1001 = total['biz_id']\n",
    "# we get the bussiness id\n",
    "test = [(len(total_rest[i]) *  testing1001[i]) for i in range(len(testing1001))]\n",
    "# get the equal number of reviews and bussiness ids\n",
    "split_mul_string = [splitstring(test[i]) for i in range(len(test))]\n",
    "# slit the business ids to one list\n",
    "sp_mul_str = list(itertools.chain.from_iterable(split_mul_string))\n",
    "sp_mul_str_df = pd.DataFrame(sp_mul_str)\n",
    "#make the bussiness id as a dataframe\n",
    "sp_mul_str_df = sp_mul_str_df.rename(columns = {0:\"biz_id\"})\n",
    "score_df.reset_index(drop=True, inplace=True)\n",
    "review_df.reset_index(drop=True, inplace=True)\n",
    "df_y = pd.concat([score_df, review_df], axis=1)\n",
    "df_y.reset_index(drop=True, inplace=True)\n",
    "sp_mul_str_df.reset_index(drop=True, inplace=True)\n",
    "df_x = pd.concat([review_df, sp_mul_str_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything = pd.read_csv(\"/Users/bckou/Documents/everything.csv\",index_col=0)\n",
    "col_attr = everything[\"attributes\"]\n",
    "\n",
    "# the function to format each restaurant's attributes into a dataframe with name of attributes as the column name. \n",
    "def format_attribute_for_list(num,attr):\n",
    "    bbb = ast.literal_eval(attr[num])\n",
    "    if (bbb[0]==\"biz_id\"): \n",
    "        ccc = pd.DataFrame(bbb)\n",
    "        ccc.columns = ccc.iloc[0]\n",
    "        ccc = ccc.iloc[1:]\n",
    "    else:\n",
    "        bbb = pd.DataFrame(bbb)\n",
    "        ccc = pd.DataFrame(bbb.T)\n",
    "        \n",
    "        ccc.columns = ccc.iloc[0]\n",
    "        ccc = pd.DataFrame(ccc)\n",
    "        ccc = ccc.iloc[1:]\n",
    "   # except (ValueError,IndexError):\n",
    "    #    ccc = pd.DataFrame([\"NA\"])\n",
    "    return ccc\n",
    "\n",
    "# get a list of dataframes: 1000 observations\n",
    "list_attribute = [format_attribute_for_list(obs,col_attr) for obs in range(0,1000)]\n",
    "\n",
    "# append all 1000 dataframes together into one dataframe. \n",
    "all_pd = pd.DataFrame().append(list_attribute)\n",
    "\n",
    "all_pd.to_csv(r'all_attributes.csv') # save the result into a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the revelant files\n",
    "review = pd.read_csv(\"/Users/bckou/Documents/review+id+score.csv\")\n",
    "attributes = pd.read_csv(\"/Users/bckou/Documents/all_attributes.csv\")\n",
    "ny_rst = pd.read_csv(\"../Documents/ny_rst.csv\")\n",
    "\n",
    "# get the id, rating and review count for each reataurant.\n",
    "ny_more = ny_rst[[\"review_count\",\"price\",\"id\"]]\n",
    "ny_more = ny_more.rename(columns={\"id\":\"biz_id\"})\n",
    "ny_more[\"price_num\"] = [len(ny_more[\"price\"][i]) if type(ny_more[\"price\"][i]) == str else None for i in range(len(ny_more))]\n",
    "\n",
    "\n",
    "# merge the review information, rating, review count and all the attributes together into one dataframe\n",
    "all_data = pd.merge(review, attributes, on='biz_id')\n",
    "model_dat = pd.merge(all_data, ny_more, on='biz_id')\n",
    "\n",
    "# save the dataframe as an excel file. \n",
    "model_dat.to_excel(r\"model_dat.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of missing values in each column(different attributes)\n",
    "\n",
    "# fill all the NaN with Missing. \n",
    "filled_model_dat = model_dat.replace(np.nan, 'Missing', regex=True)\n",
    "filled_model_dat.columns\n",
    "\n",
    "aaa = pd.DataFrame(model_dat.isnull().sum(axis = 0))\n",
    "aaa.drop(aaa.index[0], inplace=True)\n",
    "aaa.columns = [\"number of NA\"]\n",
    "aaa.index.name = \"attributes\"\n",
    "bbb = aaa.reset_index()\n",
    "ccc = bbb.sort_values(by = [\"number of NA\"],ascending = True)\n",
    "ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dat = pd.read_excel(\"/Users/bckou/Documents/model_dat1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the random forest model\n",
    "\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# replace Yes; No with numbers \n",
    "new_model_dat = model_dat.replace(\"Yes\",1)\n",
    "new_model_dat = new_model_dat.replace(\"Free\",1)\n",
    "new_model_dat = new_model_dat.replace(\"No\",0)\n",
    "new_model_dat = new_model_dat.replace(\"Paid\",0)\n",
    "\n",
    "new_model_dat = new_model_dat.fillna(value = 0)\n",
    "\n",
    "X = new_model_dat[[\"Takes Reservations\",\"Accepts Credit Cards\",\"Take-out\",\"Accepts Apple Pay\",\n",
    "                   \"Delivery\",\"Has TV\",\"Good for Kids\",\"Outdoor Seating\",\"Good for Groups\",\"Wi-Fi\",\n",
    "                   \"Score\",\"price_num\",\"review_count\",\"Bike Parking\"]].values \n",
    "X = X.astype(int)\n",
    "y = new_model_dat.iloc[:, 53].values\n",
    "\n",
    "# divide the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Algorithm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def random_forest_regression(num):\n",
    "    \n",
    "    regressor = RandomForestRegressor(n_estimators=num, random_state=0)  \n",
    "    regressor.fit(X_train, y_train)  \n",
    "    y_pred = regressor.predict(X_test)  \n",
    "\n",
    "    # Evaluating the Algorithm\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return [MAE,MSE,RMSE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = pd.DataFrame([random_forest_regression(num) for num in range(1,100) ])\n",
    "model_result.columns = ['MAE', 'MSE',\"RMSE\"]\n",
    "model_result[\"num\"] = [i for i in range(1,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=75, random_state=0)  \n",
    "regressor.fit(X_train, y_train)  \n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred = round(y_pred,1)\n",
    "y_res = y_pred - pd.DataFrame(y)\n",
    "(ggplot(y_pred)+ geom_histogram(aes(\"y_pred\"),bins = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = y_pred - pd.DataFrame(y)\n",
    "(ggplot(y_res)+ geom_histogram(aes(\"y_res\"),bins = 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the MAE, MSE and RMSE plots against the number of estimators to get the best estimator number.\n",
    "new = model_result[[\"num\",'MAE', 'MSE',\"RMSE\"]]\n",
    "\n",
    "print(\n",
    "ggplot(new)+\n",
    "geom_point(aes(x = \"num\", y = \"MAE\"), color = \"red\"))\n",
    "\n",
    "print(\n",
    "ggplot(new)+\n",
    "geom_point(aes(x = \"num\", y = \"MSE\"), color = \"green\"))\n",
    "\n",
    "print(\n",
    "ggplot(new)+\n",
    "geom_point(aes(x = \"num\", y = \"RMSE\"), color = \"blue\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
