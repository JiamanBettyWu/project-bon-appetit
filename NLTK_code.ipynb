{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv(\"/Users/apple/Desktop/everything_7.csv\")\n",
    "reviews = total['review']\n",
    "bus_id = total['biz_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import xlrd\n",
    "import string\n",
    "import nltk.corpus\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "import nltk.corpus as corpus\n",
    "nltk.download('vader_lexicon')\n",
    "import re\n",
    "\n",
    "stopwords = corpus.stopwords.words(\"english\")\n",
    "\n",
    "import ast \n",
    "from statistics import mean\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def get_adj_and_adv(text):\n",
    "    \"\"\"\n",
    "    This functionis to firstly tokenize the words and then select\n",
    "    the words that is tagged as adverbe and adjective\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    text_token = word_tokenize(text_lower)\n",
    "    result_tags = nltk.pos_tag(text_token)\n",
    "    \n",
    "    words = [(word) for word, tag in result_tags if tag in ('JJ','RB')]\n",
    "    return (words)\n",
    "\n",
    "def get_noun(text):\n",
    "    \"\"\"\n",
    "    This functino is to tokenize the words and select the words\n",
    "    that is tagged as noun\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    text_token = word_tokenize(text_lower)\n",
    "    result_tags = nltk.pos_tag(text_token)\n",
    "    \n",
    "    words = [(word) for word, tag in result_tags if tag in ('NN')]\n",
    "    return (words)\n",
    "\n",
    "def nltk_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    This function is to process the sentiment on each tokenized sentences\n",
    "    and then generate a sentiment value for each sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def get_compound(review_total_restaurant):\n",
    "    \"\"\" \n",
    "        This function is to get the compound value, which is the weight value of negative, positive and netural words\n",
    "        for each restaurant\n",
    "    \"\"\"\n",
    "    if len(review_total_restaurant) == 0: # test if there is a review if not, the value would return 0\n",
    "        x2 = 0\n",
    "    else:\n",
    "        x = [nltk_sentiment(review_total_restaurant[i]) for i in range(len(review_total_restaurant))] \n",
    "        # get the three values for each review sentences\n",
    "        x1 = [(list(x[i].items())[-1][1]) for i in range(len(x)) ]\n",
    "        # get the compund view of each reivew\n",
    "        x2 = mean(x1)\n",
    "        #calculate the average of the compound value\n",
    "    return x2\n",
    "\n",
    "def get_biz_id(reviews):\n",
    "    \"\"\"\n",
    "    This function is to get a bussiness id for each review\n",
    "    \"\"\"\n",
    "    total_rest = [ast.literal_eval(reviews[i]) for i in range(len(reviews))]\n",
    "    test = [(len(total_rest[i]) *  te[i]) for i in range(len(te))]\n",
    "    split_mul_string = [splitstring(test[i]) for i in range(len(test))]\n",
    "    sp_mul_str = list(itertools.chain.from_iterable(split_mul_string))\n",
    "    result = pd.DataFrame(sp_mul_str)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing666 = pd.DataFrame(total_rest)\n",
    "# we first conver the reviews to a dataframe\n",
    "testing667 = testing666[testing666.columns[1:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "# Then we join all the colouns (reviews) of each row(restruant) to\n",
    "# a one colmun dataframe\n",
    "\n",
    "testing668 = pd.DataFrame(testing667)\n",
    "# new we convert the combined reviews as a dataframe\n",
    "\n",
    "testing668 = testing668.rename(columns={0: \"review\"})\n",
    "# rename it \n",
    "\n",
    "bus_id.reset_index(drop=True, inplace=True)\n",
    "testing668.reset_index(drop=True, inplace=True)\n",
    "\n",
    "testing669 = pd.concat([testing668, bus_id], axis=1)\n",
    "\n",
    "rew_tok = [sent_tokenize(testing667[i]) for i in range(len(testing667))]\n",
    "# now tokenize each sentences of the review to parper for the \n",
    "#sentiment analysis\n",
    "x2 = [get_compound(rew_tok[i]) for i in range(len(rew_tok))]\n",
    "# get the average sentiment analysis result for each resutrant\n",
    "\n",
    "compd =pd.DataFrame(x2)\n",
    "compd = compd.rename(columns={0: \"Score\"})\n",
    "#Convert the sentiment analysis result to a dataframe\n",
    "\n",
    "compd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_review_w_score = pd.concat([testing669,compd],axis = 1) \n",
    "#join it as a dataframe \n",
    "# the dataframe contains the combined reviews for each restruant and the \n",
    "#average sentiment scores for each restruant reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freqneucy words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev =  [re.sub(r'[^\\w\\s]','',reviews[i]) for i in range(len(reviews))] # tokenize by words\n",
    "# now to tokenize the scentence\n",
    "\n",
    "review_noun = [get_noun(rev[i]) for i in range(len(rev))] # get the noun for \n",
    "\n",
    "review_adj = [get_adj_and_adv(rev[i]) for i in range(len(rev))]\n",
    "\n",
    "review_noun_total = [j for i in review_noun for j in i]\n",
    "# combine the nouns to one list\n",
    "review_adj_total = [j for i in review_adj for j in i]\n",
    "#combine the adjs to one list\n",
    "\n",
    "text_token = [word_tokenize(rev[i]) for i in range(len(rev))]\n",
    "\n",
    "text_token_total = [j for i in text_token for j in i]\n",
    "\n",
    "\n",
    "# these are the stop words we want to remove \n",
    "exclude_noun = ['i','food']\n",
    "exclude_adj = ['not','i','so','very','here','just','also','really','too','well','other','first','','restaurant','next','then','as','lo','as','again']\n",
    "review_noun_total_re_stopwords = [w for w in review_noun_total if w  not in exclude_noun ]\n",
    "\n",
    "review_adh_total_re_stopwords = [w for w in review_adj_total if w  not in exclude_adj ]\n",
    "fd_n = nltk.FreqDist(review_noun_total_re_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sentiment values for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rest = [ast.literal_eval(reviews[i]) for i in range(len(reviews))]\n",
    "total_score_compound = [[get_compound(x) for x in total_review_token[i]] for i in range(len(total_review_token)) ]\n",
    "# this part is to get the vander value for each review and get each\n",
    "#review text\n",
    "testing26 = list(itertools.chain.from_iterable(total_rest))\n",
    "# compress the lists into one list\n",
    "testing25 = list(itertools.chain.from_iterable(total_score_compound))\n",
    "review_df = pd.DataFrame(testing26)\n",
    "review_df = review_df.rename(columns={0: \"Review\"})\n",
    "score_df = pd.DataFrame(testing25)\n",
    "score_df = score_df.rename(columns={0: \"Compound Value\"})\n",
    "testing1001 = total['biz_id']\n",
    "# we get the bussiness id\n",
    "test = [(len(total_rest[i]) *  testing1001[i]) for i in range(len(testing1001))]\n",
    "# get the equal number of reviews and bussiness ids\n",
    "split_mul_string = [splitstring(test[i]) for i in range(len(test))]\n",
    "# slit the business ids to one list\n",
    "sp_mul_str = list(itertools.chain.from_iterable(split_mul_string))\n",
    "sp_mul_str_df = pd.DataFrame(sp_mul_str)\n",
    "#make the bussiness id as a dataframe\n",
    "sp_mul_str_df = sp_mul_str_df.rename(columns = {0:\"biz_id\"})\n",
    "score_df.reset_index(drop=True, inplace=True)\n",
    "review_df.reset_index(drop=True, inplace=True)\n",
    "df_y = pd.concat([score_df, review_df], axis=1)\n",
    "df_y.reset_index(drop=True, inplace=True)\n",
    "sp_mul_str_df.reset_index(drop=True, inplace=True)\n",
    "df_x = pd.concat([review_df, sp_mul_str_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
